---
title: The New Microchip
date: 2024-02-15
description: LLMs are our generation's microchip.
---

In 1959, Robert Noyce invented the microchip at Fairchild Semiconductor,
ushering in the computer revolution that has been continuously unfolding ever
since. The microchip made it possible to build smaller, cheaper, more powerful
computers, paving the way for a generation of new consumer electronics devices -
in the beginning, simple products like calculators and digital watches,
eventually much more consequential technologies like the PC and the smartphone.

Nine years later, Bob Noyce and Gordon Moore went on to found Intel, Fairchild’s
spiritual successor. Intel’s chips were an integral part of the personal
computing revolution. They could be found in the Apple Macintosh and IBM PCs, in
cloud data centers, and in all manner of other consumer electronics.

Gordon Moore famously predicted that the number of transistors in an integrated
circuit would double roughly every two years - Moore’s Law. Thanks largely to
his own efforts, this prediction came true, and computing power has improved at
an exponential rate for the last fifty years, enabling applications today that
would have been unthinkable a few decades ago.

One such application is the neural network. In Yann LeCun et al.’s seminal 1989
[paper on backpropagation](http://yann.lecun.com/exdb/publis/pdf/lecun-89e.pdf),
which represents the practical birth of deep learning, they trained a tiny
~1,000 parameter neural network for handwritten digit recognition. The training
took three entire days to train on a state-of-the-art workstation. Today it
takes less than 2 minutes on a Macbook Air.

As much as anything else, Moore’s law paved the path from these tiny toy neural
networks to today’s large language models.

I see a lot of parallels between the microchip and the LLM:

1. I expect that LLMs and deep neural networks writ large will probably improve
   every single year - perhaps exponentially - just like microchips did. Just as
   compute got cheaper and more powerful, so will intelligence.
2. I expect that LLMs will be a key ingredient in a generation of new consumer
   applications, just like microchips were a key ingredient for personal
   computers and smartphones.
3. I expect, however, that the companies who invent those applications will not
   be the same companies who build the LLMs, just as Intel did invent the PC.
4. I expect that building this next generation of applications will be largely a
   matter of creating better user experiences and helping average people to
   integrate this powerful technology deeply into their lives and their work,
   just as PCs helped average people harness the power of computers.

I think that point #4 represents a massive opportunity, and that the next crop
of generational technology companies will be built by solving this problem.

To extend the analogy between microchips and LLMs, I think ChatGPT is to the LLM
as the calculator was to the microchip - merely the first, simple consumer
application of a technology that will pale in comparison to future applications
like the PC. The PC for the LLM era has not yet been built.

ChatGPT, like a pocket calculator, is profoundly useful and a massive upgrade
over what was previously available, but it’s also very limited if you think
about it:

- It only speaks when spoken to. It will never proactively do anything for you.
  It won’t summarize what you’ve missed at work while you were on vacation. It
  won’t sit on the phone with your utility company for you. It won’t pay bills
  on your behalf.
- It doesn’t have access to data about your life and work, at least not by
  default. We’re starting to solve this with tools, function calling, and
  retrieval augmented generation (RAG), etc. but these are fairly limited and
  mostly just available to developers who program their own assistants.
- It can’t truly learn over time and take into account large amounts of
  contextual information in the fluid way that a human can. RAG, fine-tuning,
  and enlarging context windows aren’t quite an adequate solution here. A human
  interlocutor doesn’t need any of these tricks to remember its last interaction
  with you. We just intuitively recall all relevant details because our neurons
  update in realtime.
- It can only do one thing at a time. One of the most important features of
  modern computers is that they are multithreaded. You can listen to Spotify and
  check your email at the same time. Why shouldn’t your LLM assistant be
  multithreaded too? Shouldn’t it be simultaneously responding to your emails,
  managing your finances, and helping to get you that elusive reservation at
  Polo Bar?

So, to summarize, the next evolution of AI assistants needs to be:

- Proactive
- Integrated with external systems
- Contextually aware
- Multithreaded

This isn’t an exhaustive list, of course. Portability and multimodality are
helpful too, which is why we’re seeing an explosion of new wearable AI devices
like [Rabbit R1](https://www.rabbit.tech/) and the
[Humane AI Pin](https://hu.ma.ne/) that you can talk to as you go about your
daily life.

All of these attributes are even more important in a B2B or enterprise setting.
In fact, I suspect that the upside for LLMs in B2B applications is even greater
than the upside in consumer applications.

Giving your employees a creative assistant like Github Copilot or ChatGPT is all
well and good, but the human being at the keyboard is still the rate limiting
factor. The real value will be created by embedding LLMs deep inside of your
business’s digital infrastructure and allowing them to proactively perform
thousands of actions in the background.

> Whenever logical processes of thought are employed - that is, whenever thought
> for a time runs along an accepted groove - there is an opportunity for the
> machine.

\- Vannevar Bush, _As We May Think_, July 1945

Every business in America is full of processes that are formulaic enough that
they can now be done by an LLM, and for formulaic processes, an LLM is a perfect
fit. Their precise instructions can be tweaked and introspected. Their outputs
can be conformed to the correct format. And, perhaps most importantly, they are
lightning fast and dirt cheap compared to humans.

But an assistant like ChatGPT is not the right vehicle for automating these
formulaic processes. We need something more - something that is proactive,
integrated with external systems, contextually aware, and multithreaded.

I believe that solving these problems can unlock the power of AI for the
American economy, the same way that graphical user interfaces unlocked the power
of computers and web browsers unlocked the power of the internet. The company
that invents these new capabilities has the potential to transform the business
world and reap huge rewards.

And, as mentioned, this company is unlikely to be Open AI or Mistral or Google
or Microsoft, because history has shown that the company that creates the
platform isn’t usually the company that builds the killer app on the platform.

- Intel built the microchip, but they didn’t build the PC (Apple, IBM, and
  Microsoft did)
- Oracle built the database, but they didn’t build the CRM (Salesforce did)
- Apple built the smartphone, but they didn’t build ridesharing (Uber did)

So that’s what I’ll be working on building. We’ll see what happens.
